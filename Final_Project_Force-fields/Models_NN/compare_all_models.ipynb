{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0684d05d",
   "metadata": {},
   "source": [
    "# COMPARE ALL NN MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d11c1a",
   "metadata": {},
   "source": [
    "### PERFORMANCE OF THE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_performance_summary(training_historyGRU, training_historyLSTM, training_historyLSTM_attention, training_historySNN_attention, number_of_timesteps_for_average = 100, figsize=(20,20)):\n",
    "    \"\"\"Plot the learning performance of the deep learning network.\n",
    "    \n",
    "    Input:\n",
    "    training_history: dictionary with training history, typically obtained from train_deep_learning_network()\n",
    "    number_of_timesteps_for_average: length of the average [positive integer number]\n",
    "    figsize: figure size [list of two positive numbers]\n",
    "        \n",
    "    Output: none\n",
    "    \"\"\"    \n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from numpy import convolve, ones\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.subplot(5, 1, 1)\n",
    "    #plt.semilogy(training_history['MSE'], 'k')\n",
    "    plt.semilogy(convolve(training_historyGRU['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'r', label=\"GRU\")\n",
    "    plt.semilogy(convolve(training_historyLSTM['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'b', label = \"LSTM\")\n",
    "    plt.semilogy(convolve(training_historyLSTM_attention['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'g', label = \"LSTM with attention\")\n",
    "    plt.semilogy(convolve(training_historySNN_attention['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'black', label = \"RNN with attention\")\n",
    "    plt.ylabel('MSE', fontsize=24)\n",
    "    plt.xlabel('Epochs', fontsize=24)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.semilogy(convolve(training_historyGRU['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'r', label=\"GRU: {}\".format(round(sum(training_historyGRU['MSE'])/len(training_historyGRU['MSE']), 5)))\n",
    "    plt.semilogy(convolve(training_historyLSTM['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'b', label = \"LSTM: {}\".format(round(sum(training_historyLSTM[\"MSE\"])/len(training_historyLSTM[\"MSE\"]), 5)))\n",
    "    plt.semilogy(convolve(training_historyLSTM_attention['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'g', label = \"LSTM with attention: {}\".format(round(sum(training_historyLSTM_attention[\"MSE\"])/len(training_historyLSTM_attention[\"MSE\"]), 5)))\n",
    "    plt.semilogy(convolve(training_historySNN_attention['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'black', label = \"RNN with attention: {}\".format(round(sum(training_historySNN_attention[\"MSE\"])/len(training_historySNN_attention[\"MSE\"]), 5)))\n",
    "    plt.ylabel('MSE', fontsize=24)\n",
    "    plt.xlabel('Epochs', fontsize=24)\n",
    "    plt.legend()\n",
    "    plt.xlim([3000, 6000])\n",
    "\n",
    "    plt.subplot(5, 1, 3)\n",
    "    #plt.semilogy(training_history['MAE'], 'k')\n",
    "    plt.semilogy(convolve(training_historyGRU['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'r', label=\"GRU\")\n",
    "    plt.semilogy(convolve(training_historyLSTM['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'b', label = \"LSTM\")\n",
    "    plt.semilogy(convolve(training_historyLSTM_attention['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'g', label = \"LSTM with attention\")\n",
    "    plt.semilogy(convolve(training_historySNN_attention['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'black', label = \"RNN with attention\")\n",
    "    plt.ylabel('MAE', fontsize=24)\n",
    "    plt.xlabel('Epochs', fontsize=24)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.semilogy(convolve(training_historyGRU['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'r', label=\"GRU: {}\".format(round(sum(training_historyGRU['MAE'])/len(training_historyGRU['MAE']), 5)))\n",
    "    plt.semilogy(convolve(training_historyLSTM['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'b', label = \"LSTM: {}\".format(round(sum(training_historyLSTM[\"MAE\"])/len(training_historyLSTM[\"MAE\"]), 5)))\n",
    "    plt.semilogy(convolve(training_historyLSTM_attention['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'g', label = \"LSTM with attention: {}\".format(round(sum(training_historyLSTM_attention[\"MAE\"])/len(training_historyLSTM_attention[\"MAE\"]), 5)))\n",
    "    plt.semilogy(convolve(training_historySNN_attention['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'black', label = \"RNN with attention: {}\".format(round(sum(training_historySNN_attention[\"MAE\"])/len(training_historySNN_attention[\"MAE\"]), 5)))\n",
    "    plt.ylabel('MAE', fontsize=24)\n",
    "    plt.xlabel('Epochs', fontsize=24)\n",
    "    plt.legend()\n",
    "    plt.xlim([3000, 6000])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44de9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload results\n",
    "\n",
    "training_history_GRU, \n",
    "training_history_LSTM, \n",
    "training_history_LSTM_attention, \n",
    "training_history_RNN_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd75fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot summary of the learning performance\n",
    "number_of_timesteps_for_average = 100\n",
    "plot_learning_performance_summary(training_history_GRU, training_history_LSTM, training_history_LSTM_attention, training_history_RNN_attention, number_of_timesteps_for_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ca7701",
   "metadata": {},
   "source": [
    "#### PREDICTION OF THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddde1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
