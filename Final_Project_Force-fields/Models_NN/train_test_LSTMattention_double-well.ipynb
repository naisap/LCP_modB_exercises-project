{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0a5ace",
   "metadata": {},
   "source": [
    "### GENERATE TRAJECTORIES USING **DOUBLE-WELL POTENTIAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18d2551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.constants import Boltzmann as kB\n",
    "from numpy.random import randn as gauss\n",
    "from numpy.random import rand as uniform\n",
    "\n",
    "### Physical parameters \n",
    "R = 1e-7                                # Radius of the Brownian particle [m]\n",
    "eta = 0.001                             # Viscosity of the medium [kg m^-1 s^-1]\n",
    "T = 300                                 # Temperature [K]\n",
    "L0 = 2e-6                               # Reference distance from middle to one minimum [m]\n",
    "H0 = kB*300                             # Barrier height [Joule]\n",
    "gamma0 = 6 * np.pi * eta * R            # Reference friction coefficient [kg s^-1]\n",
    "\n",
    "### Simulation parameters\n",
    "N = 1000                   # Number of samples of the trajectory\n",
    "Dt = 1e-2                  # Timestep \n",
    "oversampling = 5           # Simulation oversampling\n",
    "offset = 1000              # Number of equilibration timesteps\n",
    "batch_size = 32            # Number of trajectories\n",
    "\n",
    "### Define functions to scale and rescale inputs\n",
    "scale_inputs = lambda x: x * 1e+6                    # Scales input trajectory to order 1\n",
    "rescale_inputs = lambda scaled_x: scaled_x * 1e-6    # Rescales input trajectory to physical units\n",
    "\n",
    "### Define function to scale and rescale targets\n",
    "scale_targets = lambda L, H: [L/L0 -1, np.log(H / H0)]                        # Scales targets to order 1\n",
    "rescale_targets = lambda scaled_L, scaled_H: [(1 + scaled_L)*L0*1e6, \n",
    "                                              np.exp(scaled_H) * H0/kB/300] # Inverse of targets_scaling\n",
    "\n",
    "def simulate_trajectory(batch_size=batch_size, \n",
    "                        T=T,\n",
    "                        H0=H0,\n",
    "                        L0=L0,\n",
    "                        gamma0=gamma0,\n",
    "                        N=N, \n",
    "                        Dt=Dt, \n",
    "                        oversampling=oversampling, \n",
    "                        offset=offset):#, \n",
    "                        #scale_inputs=scale_inputs, \n",
    "                        #scale_targets=scale_targets):\n",
    "    \n",
    "    ### Randomize trajectory parameters\n",
    "    L = L0 * (uniform(batch_size)+.5) \n",
    "    H = H0 * 10**(uniform(batch_size)*1.75 - .75)       # Generates random values for computing the stiffness\n",
    "    gamma = gamma0 * (uniform(batch_size) * .1 + .95)   # Marginal randomization of friction coefficient to tolarate small changes\n",
    "\n",
    "    ### Simulate\n",
    "    dt = Dt / oversampling                 # time step of the simulation\n",
    "    x = np.zeros((batch_size, N))          # initialization of the x array\n",
    "    k0 = 4*H/L**2 \n",
    "    k1 = 4*H/L**4\n",
    "    D = kB * T / gamma                     # diffusion coefficient\n",
    "    C1 = +k0 / gamma * dt\n",
    "    C2 = -k1 / gamma * dt                  # drift coefficient of the Langevin equation\n",
    "    C3 = np.sqrt(2 * D * dt)               # random walk coefficient of the Langevin equation\n",
    "    X = x[:, 0]\n",
    "    n = 0\n",
    "    \n",
    "    for t in range(offset):                      # Offset (for some prerun before running)\n",
    "        X = X + C1 * X + C2 * X**3 + C3 * gauss(batch_size)\n",
    "        #X = X - (2 * X**3) + (12 * X**2) - (18 * X) + 3 + (C3 * gauss(batch_size))\n",
    "        \n",
    "    for t in range(N * oversampling):            # Simulation                \n",
    "        X = X + C1 * X + C2 * X**3 + C3 * gauss(batch_size)\n",
    "        #X = X - (2 * X**3) + (12 * X**2) - (18 * X) + 3 + (C3 * gauss(batch_size))\n",
    "        if t % oversampling == 0:                # We save every oversampling^th values \n",
    "            x[:, n] = X \n",
    "            n += 1\n",
    "\n",
    "    inputs = scale_inputs(x)\n",
    "    targets = np.swapaxes(scale_targets(*[L, H]),0,1)\n",
    "    target_reals = np.swapaxes([L*1e6, H/kB/300],0,1)\n",
    "\n",
    "    return inputs, targets, target_reals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e8c03",
   "metadata": {},
   "source": [
    "##### TRAIN THE DEEP NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a563a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_deep_learning_network(\n",
    "    network,\n",
    "    simulate_trajectory,\n",
    "    sample_sizes = (32, 128, 512),#(32, 128, 512, 2048),\n",
    "    iteration_numbers = (3001, 2001, 1001),#(1001, 2001, 3001),#(3001, 2001, 1001, 101),\n",
    "    verbose=.1):\n",
    "    \"\"\"Train a deep learning network.\n",
    "    \n",
    "    Input:\n",
    "    network: deep learning network\n",
    "    simulate_trajectory: trajectory generator function\n",
    "    sample_sizes: sizes of the batches of trajectories used in the training [tuple of positive integers]\n",
    "    iteration_numbers: numbers of batches used in the training [tuple of positive integers]\n",
    "    verbose: frequency of the update messages [number between 0 and 1]\n",
    "        \n",
    "    Output:\n",
    "    training_history: dictionary with training history\n",
    "    \"\"\"  \n",
    "    \n",
    "    import numpy as np\n",
    "    from time import time\n",
    "     \n",
    "    training_history = {}\n",
    "    training_history['Sample Size'] = []\n",
    "    training_history['Iteration Number'] = []\n",
    "    training_history['Iteration Time'] = []\n",
    "    training_history['MSE'] = []\n",
    "    training_history['MAE'] = []\n",
    "    \n",
    "    for sample_size, iteration_number in zip(sample_sizes, iteration_numbers):\n",
    "        for iteration in range(iteration_number):\n",
    "            \n",
    "            # measure initial time for iteration\n",
    "            initial_time = time()\n",
    "\n",
    "            # generate trajectories and targets\n",
    "            network_blocksize = network.get_layer(index=0).get_config()['batch_input_shape'][1:][1]                        \n",
    "            number_of_outputs = network.get_layer(index=-1).get_config()['units']\n",
    "            output_shape = (sample_size, number_of_outputs)\n",
    "            targets = np.zeros(output_shape)\n",
    "            \n",
    "            \n",
    "            batch_size = sample_size\n",
    "            trajectory, target, target_real = simulate_trajectory(batch_size)\n",
    "            #trajectory = trajectory.scaled_values\n",
    "            trajectory_dimensions = [sample_size, round(trajectory.size/network_blocksize/sample_size), network_blocksize]\n",
    "            trajectories = np.array(trajectory).reshape(trajectory_dimensions)\n",
    "            targets = target#.scaled_values\n",
    "                \n",
    "                \n",
    "\n",
    "            # training\n",
    "            history = network.fit(trajectories,\n",
    "                                targets,\n",
    "                                epochs=1, \n",
    "                                batch_size=sample_size,\n",
    "                                verbose=False)\n",
    "                        \n",
    "            # measure elapsed time during iteration\n",
    "            iteration_time = time() - initial_time\n",
    "\n",
    "            # record training history\n",
    "            mse = history.history['mse'][0]\n",
    "            mae = history.history['mae'][0]\n",
    "                        \n",
    "            training_history['Sample Size'].append(sample_size)\n",
    "            training_history['Iteration Number'].append(iteration)\n",
    "            training_history['Iteration Time'].append(iteration_time)\n",
    "            training_history['MSE'].append(mse)\n",
    "            training_history['MAE'].append(mae)\n",
    "\n",
    "            if not(iteration%int(verbose**-1)):\n",
    "                print('Sample size %6d   iteration number %6d   MSE %10.4f   MAE %10.4f   Time %10f ms' % (sample_size, iteration + 1, mse, mae, iteration_time * 1000))\n",
    "                \n",
    "    return training_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8778b2",
   "metadata": {},
   "source": [
    "##### PLOT THE PERFORMANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a67c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_performance(training_history, number_of_timesteps_for_average = 100, figsize=(20,20)):\n",
    "    \"\"\"Plot the learning performance of the deep learning network.\n",
    "    \n",
    "    Input:\n",
    "    training_history: dictionary with training history, typically obtained from train_deep_learning_network()\n",
    "    number_of_timesteps_for_average: length of the average [positive integer number]\n",
    "    figsize: figure size [list of two positive numbers]\n",
    "        \n",
    "    Output: none\n",
    "    \"\"\"    \n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from numpy import convolve, ones\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.semilogy(training_history['MSE'], 'k')\n",
    "    plt.semilogy(convolve(training_history['MSE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'r')\n",
    "    plt.ylabel('MSE', fontsize=24)\n",
    "    plt.xlabel('Epochs', fontsize=24)\n",
    "\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.semilogy(training_history['MAE'], 'k')\n",
    "    plt.semilogy(convolve(training_history['MAE'], ones(number_of_timesteps_for_average) / number_of_timesteps_for_average, mode='valid'), 'r')\n",
    "    plt.ylabel('MAE', fontsize=24)\n",
    "    plt.xlabel('Epochs', fontsize=24)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda8119",
   "metadata": {},
   "source": [
    "##### TEST THE DNN ON NEW SIMULATED TRAJECTORIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1daf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, trajectory):\n",
    "    \"\"\" Predict parameters of the force field from the trajectory using the deep learnign network.\n",
    "    \n",
    "    Inputs:\n",
    "    network: deep learning network\n",
    "    image: trajectroy [numpy array of real numbers]\n",
    "    \n",
    "    Output:\n",
    "    predicted_targets: predicted parameters of the calibrated force field [1D numpy array containing outputs]\n",
    "    \"\"\"\n",
    "    \n",
    "    from numpy import reshape\n",
    "    \n",
    "    network_blocksize = network.get_layer(index=0).get_config()['batch_input_shape'][1:][1]\n",
    "    predicted_targets = network.predict(reshape(trajectory, [1,round(trajectory.size/network_blocksize),network_blocksize]))   \n",
    "        \n",
    "    return predicted_targets\n",
    "\n",
    "\n",
    "def test_performance(simulate_trajectory, network, rescale_targets, number_of_predictions_to_show=100):#, dt = 1e-1):\n",
    "\n",
    "    \n",
    "    network_blocksize = network.get_layer(index=0).get_config()['batch_input_shape'][1:][1]\n",
    "\n",
    "\n",
    "    predictions_scaled = []\n",
    "    predictions_physical = []\n",
    "\n",
    "    batch_size = number_of_predictions_to_show\n",
    "    trajectory, targets, targets_real = simulate_trajectory(batch_size)\n",
    "    targets_physical = list(targets_real)#targets.values)\n",
    "    targets_scaled = list(targets)#.scaled_values)\n",
    "    #trajectory = trajectory.scaled_values\n",
    "    trajectory_dimensions = [number_of_predictions_to_show, round(trajectory.size/network_blocksize/number_of_predictions_to_show) , network_blocksize]\n",
    "    trajectories = np.array(trajectory).reshape(trajectory_dimensions)\n",
    "       \n",
    "\n",
    "    for i in range(number_of_predictions_to_show):\n",
    "        predictions = predict(network, trajectories[i])\n",
    "\n",
    "\n",
    "        predictions_scaled.append(predictions[0])\n",
    "        predictions_physical.append(rescale_targets(*predictions[0]))\n",
    "\n",
    "    number_of_outputs = network.get_layer(index=-1).get_config()['units']    \n",
    "\n",
    "    targets_physical = np.array(targets_physical).transpose()\n",
    "    targets_scaled = np.array(targets_scaled).transpose()\n",
    "    predictions_scaled = np.array(predictions_scaled).transpose()\n",
    "    predictions_physical = np.array(predictions_physical).transpose()\n",
    "\n",
    "    # Do not show results at the edges of the training range \n",
    "\n",
    "    if number_of_outputs>1:\n",
    "\n",
    "        ind = np.isfinite(targets_scaled[0])\n",
    "        for target_number in range(number_of_outputs):\n",
    "            target_max = .9 * np.max(targets_scaled[target_number]) + .1 * np.min(targets_scaled[target_number])\n",
    "            target_min = .1 * np.max(targets_scaled[target_number]) + .9 * np.min(targets_scaled[target_number])\n",
    "            ind = np.logical_and(ind, targets_scaled[target_number] < target_max)\n",
    "            ind = np.logical_and(ind, targets_scaled[target_number] > target_min)\n",
    "    else:\n",
    "        target_max = .9 * np.max(targets_scaled) + .1 * np.min(targets_scaled)\n",
    "        target_min = .1 * np.max(targets_scaled) + .9 * np.min(targets_scaled)\n",
    "        ind = np.logical_and(targets_scaled < target_max, targets_scaled > target_min)\n",
    "\n",
    "    return targets_scaled, targets_physical, predictions_scaled, predictions_physical\n",
    "\n",
    "\n",
    "def plot_test_performance(targets_scaled, targets_physical, predictions_scaled, predictions_physical, network):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    #from . import predict\n",
    "\n",
    "    number_of_outputs = network.get_layer(index=-1).get_config()['units']    \n",
    "    \n",
    "    if number_of_outputs>1:\n",
    "\n",
    "        for target_number in range(number_of_outputs):\n",
    "            plt.figure(figsize=(20, 10))\n",
    "\n",
    "            plt.subplot(121)\n",
    "            plt.plot(targets_scaled[target_number],\n",
    "                     predictions_scaled[target_number],\n",
    "                     '.')\n",
    "            #plt.xlabel(targets.scalings[target_number], fontsize=18)\n",
    "            #plt.ylabel('Predicted ' + targets.scalings[target_number], fontsize=18)\n",
    "            plt.axis('square')\n",
    "            plt.title('Prediction performance in scaled units', fontsize=18)\n",
    "\n",
    "            plt.subplot(122)\n",
    "            plt.plot(targets_physical[target_number],\n",
    "                     predictions_physical[target_number],\n",
    "                    '.')\n",
    "            #plt.xlabel(targets.names[target_number], fontsize=18)\n",
    "            #plt.ylabel('Predicted ' + targets.names[target_number], fontsize=18)\n",
    "            plt.axis('square')\n",
    "            plt.title('Prediction performance in real units', fontsize=18)\n",
    "\n",
    "\n",
    "    else: \n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.plot(targets_scaled[ind],\n",
    "                 predictions_scaled.transpose()[ind],\n",
    "                 '.')\n",
    "        #plt.xlabel(targets.scalings[0], fontsize=18)\n",
    "        #plt.ylabel('Predicted ' + targets.scalings[0], fontsize=18)\n",
    "        plt.axis('square')\n",
    "        plt.title('Prediction performance in scaled units', fontsize=18)\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.plot(targets_physical[ind],\n",
    "                 predictions_physical.transpose()[ind],\n",
    "                '.')\n",
    "        #plt.xlabel(targets.names[0], fontsize=18)\n",
    "        #plt.ylabel('Predicted ' + targets.names[0], fontsize=18)\n",
    "        plt.axis('square')\n",
    "        plt.title('Prediction performance in real units', fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8659ae50",
   "metadata": {},
   "source": [
    "## LSTM with ATTENTION\n",
    "\n",
    "CREATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9bebd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        \n",
    "        print('alpha',alpha.shape)\n",
    "        \n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        \n",
    "        print('alpha',alpha.shape)\n",
    "\n",
    "        \n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        \n",
    "        print('---',context.shape,context.shape[0])\n",
    "        print('==',tf.reshape(context, (-1, context.shape[0])))\n",
    "        print('aa', K.reshape(context, (-1, context.shape[0])))\n",
    "        \n",
    "        context = K.expand_dims(context, axis=-1)  # Add this line to make the tensor 2D\n",
    "        #context = K.reshape(context, (1, context.shape[0]))\n",
    "        #context = tf.keras.layers.Reshape(context, input_shape=(50,))\n",
    "        print(context.shape)\n",
    "        \n",
    "        #context_.set_shape([None, context.shape[0]])\n",
    "        #context_.__setattr__(\"_keras_shape\", (None,context.shape[0]))\n",
    "        #conv_x = Conv2D(16, (1,1))(context_)\n",
    "        #print(conv_x.shape)\n",
    "        return context\n",
    "    \n",
    "\n",
    "def create_deep_learning_network(\n",
    "    input_shape=(None, 50),\n",
    "    lstm_layers_dimensions=(100, 50),\n",
    "    number_of_outputs=2) :\n",
    "    \"\"\"Creates and compiles a deep learning network.\n",
    "    \n",
    "    Inputs:    \n",
    "    input_shape: Should be the same size of the input trajectory []\n",
    "    lstm_layers_dimensions: number of neurons in each LSTM layer [tuple of positive integers]\n",
    "        \n",
    "    Output:\n",
    "    network: deep learning network\n",
    "    \"\"\"    \n",
    "\n",
    "    from keras import models, layers, optimizers\n",
    "\n",
    "    ### ATTENTION LAYER\n",
    "    def attention_layer(inputs):\n",
    "        attention_units = K.int_shape(inputs)[-1]\n",
    "        attention_weights = layers.Dense(attention_units, activation='softmax')(inputs)\n",
    "        weighted_inputs = layers.Multiply()([inputs, attention_weights])\n",
    "        return weighted_inputs\n",
    "    \n",
    "    ### INITIALIZE DEEP LEARNING NETWORK\n",
    "    network = models.Sequential()\n",
    "\n",
    "    ### CONVOLUTIONAL BASIS\n",
    "    for lstm_layer_number, lstm_layer_dimension in zip(range(len(lstm_layers_dimensions)), lstm_layers_dimensions):\n",
    "\n",
    "        # add LSTM layer\n",
    "        lstm_layer_name = 'lstm_' + str(lstm_layer_number + 1)\n",
    "        if lstm_layer_number + 1 < len(lstm_layers_dimensions): # All layers but last\n",
    "            lstm_layer = layers.LSTM(lstm_layer_dimension,\n",
    "                                     return_sequences=True,\n",
    "                                     dropout=0,\n",
    "                                     recurrent_dropout=0,\n",
    "                                     input_shape=input_shape,\n",
    "                                     name=lstm_layer_name)\n",
    "\n",
    "        else: # Last layer\n",
    "            lstm_layer = layers.LSTM(lstm_layer_dimension,\n",
    "                                     return_sequences=False,\n",
    "                                     dropout=0,\n",
    "                                     recurrent_dropout=0,\n",
    "                                     input_shape=input_shape,\n",
    "                                     name=lstm_layer_name)\n",
    "\n",
    "        network.add(lstm_layer)\n",
    "    \n",
    "    # Add Attention Layer\n",
    "    print(network.output)\n",
    "    attention_lay = attention()(network.output)\n",
    "    #network.add(attention())\n",
    "    #network = Model(network.input, attention_lay)\n",
    "    \n",
    "    # OUTPUT LAYER\n",
    "    #output_layer = layers.Dense(number_of_outputs, name='output')\n",
    "    #network.add(output_layer)\n",
    "    \n",
    "    print(network.input)\n",
    "    output_layer = layers.Dense(number_of_outputs, name='output')(attention_lay)\n",
    "    #output = output_layer(attention_lay)\n",
    "    \n",
    "    network = Model(network.input, output_layer)\n",
    "    print(network.output)\n",
    "    network.compile(optimizer=optimizers.Adam(lr=1e-3), loss='mse', metrics=['mse', 'mae'])\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "30bf2a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 50), dtype=tf.float32, name=None), name='lstm_2/PartitionedCall:0', description=\"created by layer 'lstm_2'\")\n",
      "alpha (50,)\n",
      "alpha (50, 1)\n",
      "--- (50,) 50\n",
      "== Tensor(\"attention_46/Reshape:0\", shape=(1, 50), dtype=float32)\n",
      "aa Tensor(\"attention_46/Reshape_1:0\", shape=(1, 50), dtype=float32)\n",
      "(50, 1)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, None, 50), dtype=tf.float32, name='lstm_1_input'), name='lstm_1_input', description=\"created by layer 'lstm_1_input'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(50, 2), dtype=tf.float32, name=None), name='output/BiasAdd:0', description=\"created by layer 'output'\")\n",
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1_input (InputLayer)   [(None, None, 50)]        0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 100)         60400     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50)                30200     \n",
      "                                                                 \n",
      " attention_46 (attention)    (50, 1)                   100       \n",
      "                                                                 \n",
      " output (Dense)              (50, 2)                   4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,704\n",
      "Trainable params: 90,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Create deep learning network\n",
    "networkLSTMwithATTENTION = create_deep_learning_network()\n",
    "\n",
    "### Print deep learning network summary\n",
    "networkLSTMwithATTENTION.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7f9b1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.layers.reshaping.reshape.Reshape at 0x1c54ce9ab80>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assuming you have a tensor named 'tensor' with shape (50,)\n",
    "tensor = K.zeros((50,))\n",
    "print(tensor.shape)\n",
    "# Get the shape of the tensor\n",
    "#shape = K.shape(tensor)\n",
    "#print(shape, '--', shape[0])\n",
    "# Reshape the tensor to shape (None, 50)\n",
    "reshaped_tensor = tf.keras.layers.Reshape((50,), input_shape=(50,))\n",
    "#K.reshape(tensor, input_shape=(50,))\n",
    "#(3, 4), input_shape=(12,)\n",
    "reshaped_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1275a65",
   "metadata": {},
   "source": [
    "#### TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8c84129f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\fap_9\\AppData\\Local\\Temp\\__autograph_generated_filemo9iiqju.py\", line 10, in tf__call\n        e = ag__.converted_call(ag__.ld(K).tanh, (ag__.converted_call(ag__.ld(K).dot, (ag__.ld(x), ag__.ld(self).W), None, fscope) + ag__.ld(self).b,), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'attention_46' (type attention).\n    \n    in user code:\n    \n        File \"C:\\Users\\fap_9\\AppData\\Local\\Temp\\ipykernel_35044\\2743893391.py\", line 22, in call  *\n            e = K.tanh(K.dot(x,self.W)+self.b)\n    \n        ValueError: Dimensions must be equal, but are 32 and 50 for '{{node model_15/attention_46/add}} = AddV2[T=DT_FLOAT](model_15/attention_46/MatMul, model_15/attention_46/add/ReadVariableOp)' with input shapes: [32,1], [50,1].\n    \n    \n    Call arguments received by layer 'attention_46' (type attention):\n      • x=tf.Tensor(shape=(32, 50), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[2], line 53\u001b[0m, in \u001b[0;36mtrain_deep_learning_network\u001b[1;34m(network, simulate_trajectory, sample_sizes, iteration_numbers, verbose)\u001b[0m\n\u001b[0;32m     48\u001b[0m targets \u001b[38;5;241m=\u001b[39m target\u001b[38;5;66;03m#.scaled_values\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# measure elapsed time during iteration\u001b[39;00m\n\u001b[0;32m     60\u001b[0m iteration_time \u001b[38;5;241m=\u001b[39m time() \u001b[38;5;241m-\u001b[39m initial_time\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filev032ggjm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemo9iiqju.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m e \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mtanh, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39mdot, (ag__\u001b[38;5;241m.\u001b[39mld(x), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mW), \u001b[38;5;28;01mNone\u001b[39;00m, fscope) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mb,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m e \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39msqueeze, (ag__\u001b[38;5;241m.\u001b[39mld(e),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[0;32m     12\u001b[0m alpha \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(K)\u001b[38;5;241m.\u001b[39msoftmax, (ag__\u001b[38;5;241m.\u001b[39mld(e),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\fap_9\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\fap_9\\AppData\\Local\\Temp\\__autograph_generated_filemo9iiqju.py\", line 10, in tf__call\n        e = ag__.converted_call(ag__.ld(K).tanh, (ag__.converted_call(ag__.ld(K).dot, (ag__.ld(x), ag__.ld(self).W), None, fscope) + ag__.ld(self).b,), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'attention_46' (type attention).\n    \n    in user code:\n    \n        File \"C:\\Users\\fap_9\\AppData\\Local\\Temp\\ipykernel_35044\\2743893391.py\", line 22, in call  *\n            e = K.tanh(K.dot(x,self.W)+self.b)\n    \n        ValueError: Dimensions must be equal, but are 32 and 50 for '{{node model_15/attention_46/add}} = AddV2[T=DT_FLOAT](model_15/attention_46/MatMul, model_15/attention_46/add/ReadVariableOp)' with input shapes: [32,1], [50,1].\n    \n    \n    Call arguments received by layer 'attention_46' (type attention):\n      • x=tf.Tensor(shape=(32, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "training_history_LSTM_attention = train_deep_learning_network(networkLSTMwithATTENTION, simulate_trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76f61b",
   "metadata": {},
   "source": [
    "#### TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44118a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "<lambda>() missing 1 required positional argument: 'scaled_H'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 43\u001b[0m, in \u001b[0;36mtest_performance\u001b[1;34m(simulate_trajectory, network, rescale_targets, number_of_predictions_to_show)\u001b[0m\n\u001b[0;32m     39\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict(network, trajectories[i])\n\u001b[0;32m     42\u001b[0m     predictions_scaled\u001b[38;5;241m.\u001b[39mappend(predictions[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 43\u001b[0m     predictions_physical\u001b[38;5;241m.\u001b[39mappend(\u001b[43mrescale_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     45\u001b[0m number_of_outputs \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mget_layer(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mget_config()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m]    \n\u001b[0;32m     47\u001b[0m targets_physical \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(targets_physical)\u001b[38;5;241m.\u001b[39mtranspose()\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() missing 1 required positional argument: 'scaled_H'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "number_of_predictions_to_show = 1000\n",
    "prediction_LSTMwithATTENTION_test = test_performance(simulate_trajectory, networkLSTMwithATTENTION, rescale_targets, number_of_predictions_to_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720e775",
   "metadata": {},
   "source": [
    "#### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319fbd20",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot learning performance\n",
    "number_of_timesteps_for_average = 100\n",
    "plot_learning_performance(training_history_LSTM_attention, number_of_timesteps_for_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8db5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot test performance\n",
    "plot_test_performance(prediction_LSTMwithATTENTION_test[0], prediction_LSTMwithATTENTION_test[1], prediction_LSTMwithATTENTION_test[2], prediction_LSTMwithATTENTION_test[3], networkLSTM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
